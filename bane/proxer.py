import requests,socks,socket,random,re
import bs4
from bs4 import BeautifulSoup
from bane.payloads import *
from bane.pager import crawl

def proxyscrape(protocol="http",timeout=10,country="all",ssl="all",anonymity="all"):
 if protocol.lower() not in ("http","socks4","socks5","all"):
  raise Exception('protocol value must be: "http" or "socks4" or "socks5" or "all"')
 if ssl.lower() not in ("no","yes","all"):
  raise Exception('protocol value must be: "no" or "yes" or "all"')
 if anonymity.lower() not in ("elite","anonymous","transparent","all"):
  raise Exception('protocol value must be: "elite" or "anonymous" or "transparent" or "all"')
 return requests.get("https://api.proxyscrape.com/v2/?request=getproxies&protocol="+protocol+"&timeout="+str(timeout*1000)+"&country="+country+"&ssl="+ssl+"&anonymity="+anonymity ,headers={"User-Agent":random.choice(ua)}).text.split('\r\n')

def masshttp(count=None,timeout=15):
 '''
   this function gather up thousands of HTTP / HTTPS proxies from www.proxyserverlist24.top and proxy-daily.com
   those proxies are not recommended to be used as reliable ones all the time, i use them here just to distribute my attacks
   on next functions...
   if you are willing to use them please check them first!!!
   the function takes an argument (*args) which is the number of proxies to return, in case of no argument given it will
   return the whole list.
   usage:
   >>>import bane
   >>>bane.masshttp()
   >>>bane.masshttp(1500)
 '''
 if count!=None:
  m=count
 else:
  m=999999
 u="http://www.proxyserverlist24.top/#"
 t=[]
 l=[]
 h=[]
 try:
  h3tags1 = crawl(u,timeout=timeout)
  h3tags=[]
  for x in h3tags1.keys():
   h3tags.append(h3tags1[x][1])
  for a in h3tags:
    try:
     if (("proxy-server" in str(a)) and("#" not in (str(a)))) :
      try:
       if (a not in l):
          l.append(a)
      except Exception as xx:
       pass
    except Exception as ex:
     pass
     continue
 except Exception as e:
  pass
 for u in l:
  try:
   a=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout)
   ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",a.text)
   t+=ips
  except Exception as e:
   pass
 l=[]
 u="https://www.dailyfreeproxy.com/#"
 try:
  h3tags1 = crawl(u,timeout=timeout)
  h3tags=[]
  for x in h3tags1.keys():
   h3tags.append(h3tags1[x][1])
  for a in h3tags:
    try:
     if (("-http" in str(a)) and("#" not in (str(a)))) :
      try:
       a=str(a)
       if (a not in l):
          l.append(a)
      except Exception as xx:
       pass
    except Exception as ex:
     continue
 except Exception as e:
  pass
 for u in l:
  try:
   a=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout)
   ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",a.text)
   t+=ips
  except Exception as e:
   pass
  u="http://proxy-daily.com/#"
  try:
   r=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout).text
   soup= BeautifulSoup ( r, 'html.parser')
   l = soup.findAll('div')
  except:
   pass
  p=[]
  ips=[]
  for x in l:
   try:
    ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",str(x))
    if (ips) and (ips not in p):
     p.append(ips)
   except:
    pass
 try:
  t+=p[0]
 except:
  pass
 if count!=None:
  while True:
   o=random.choice(t)
   h.append(o)
   if (len(h)==m) or (len(h)==len(t)):
    break
 else:
  h=t
 return h
def massocks4(count=None,timeout=15):
 '''
   this function gather up thousands of SOCKS4 proxies from www.proxyserverlist24.top and proxy-daily.com
   those proxies are not recommended to be used as reliable ones all the time, i use them here just to distribute my attacks
   on next functions...
   if you are willing to use them please check them first!!!
   the function takes an argument (*args) which is the number of proxies to return, in case of no argument given it will
   return the whole list.

   usage:

   >>>import bane
   >>>bane.massocks4()

   >>>bane.massocks4(100)
 '''
 if count!=None:
  m=count
 else:
  m=999999
 s4=[]
 t=[]
 l=[]
 u="https://www.dailyfreeproxy.com/#"
 try:
  h3tags1 = crawl(u,timeout=timeout)
  h3tags=[]
  for x in h3tags1.keys():
   h3tags.append(h3tags1[x][1])
  for a in h3tags:
    try:
     if (("-socks4" in str(a)) and("#" not in (str(a)))) :
      try:
       a=str(a)
       if (a not in l):
          l.append(a)
      except Exception as xx:
       pass
    except Exception as ex:
     continue
 except Exception as e:
  pass
 for u in l:
  try:
   a=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout)
   ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",a.text)
   t+=ips
  except Exception as e:
   pass
 l=[]
 u="http://proxy-daily.com/#"
 try:
   r=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout).text
   soup= BeautifulSoup ( r, 'html.parser')
   l = soup.findAll('div')
 except Exception as e:
   pass
 p=[]
 for x in l:
   try:
    ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",str(x))
    if (ips) and (ips not in p):
     p.append(ips)
   except Exception as x:
    pass
 try:
  t+=p[2]
 except:
  pass
 if count!=None:
  while True:
   o=random.choice(t)
   s4.append(o)
   if (len(s4)==m) or (len(s4)==len(t)):
    break
 else:
  s4=t
 return s4
def massocks5(count=None,timeout=15):
 '''
   this function gather up thousands of SOCKS5 proxies from www.proxyserverlist24.top and proxy-daily.com
   those proxies are not recommended to be used as reliable ones all the time, i use them here just to distribute my attacks
   on next functions...
   if you are willing to use them please check them first!!!
   the function takes an argument which is the number of proxies to return (*args) , in case of no argument given it will
   return the whole list.

   usage:

   >>>import bane
   >>>bane.massocks5()

   >>>bane.massocks5(500)
 '''
 if count!=None:
  m=count
 else:
  m=999999
 l=[]
 t=[]
 u="https://www.dailyfreeproxy.com/#"
 try:
  h3tags1 = crawl(u,timeout=timeout)
  h3tags=[]
  for x in h3tags1.keys():
   h3tags.append(h3tags1[x][1])
  for a in h3tags:
    try:
     if (("-socks5" in str(a)) and("#" not in (str(a)))) :
      try:
       a=str(a)
       if (a not in l):
          l.append(a)
      except Exception as xx:
       pass
    except Exception as ex:
     continue
 except Exception as e:
  pass
 for u in l:
  try:
   a=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout)
   ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",a.text)
   t+=ips
  except Exception as e:
   pass
 u="http://www.live-socks.net/#"
 l=[]
 s5=[]
 try:
  r=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout).text
  soup= BeautifulSoup ( r, 'html.parser')
  h3 = soup.find_all( 'h3',class_='post-title entry-title' )
  for ha in h3:
   h3tags = ha.find_all('a')
   for a in h3tags:
    try:
     a=str(a)
     if("socks-5" in a):
       a=a.split('href="')[1]
       a=a.split('"')[0]
       if (a not in l):
          l.append(a)
    except Exception as ex:
     continue
 except Exception as e:
  pass
 for u in l:
  try:
   a=requests.get(u,headers={'User-Agent': random.choice(ua)},timeout=timeout)
   ips = re.findall("(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3})\.(?:[\d]{1,3}):(?:[\d]{1,5})",a.text)
   for x in ips:
    if (x not in t):
     t.append(x)
  except Exception as e:
   pass
 if count!=None:
  while True:
   o=random.choice(t)
   s5.append(o)
   if (len(s5)==m) or (len(s5)==len(t)):
    break
 else:
  s5=t
 return s5
def http(logs=True,count=300,timeout=15):
 '''
   this function gather up hundreds of HTTP proxies from many sources.
   those proxies are recommended to be used as reliable ones all the time.
   the function takes an argument which is the number of proxies to return (*args) , in case of no argument given it will
   return the whole list,and another argument (logs) which is set by default to True so you can see the process of the function, you can turn 
   off the the typing if you set it to: False.

   usage:

   >>>import bane
   >>>bane.http()

   >>>bane.http(300)
'''
 hsl=[]
 u="https://free-proxy-list.net/"
 try:
  if logs==True:
   print("[+]Checking: {}".format(u))
  c=requests.get(u,timeout=timeout).text
  a=0
  soup = BeautifulSoup(c,"html.parser")
  y=soup.find_all('tr')
  for x in y:
   try:
    a+=1
    x=str(x)
    ip=x.split('<tr><td>')[1].split('=')[0]
    ip=ip.split('</td>')[0].split('=')[0]
    p=x.split('</td><td>')[1].split('=')[0]
    p=p.split('</td>')[0].split('=')[0]
    pr=ip+':'+p
    if pr not in hsl:
     hsl.append(pr)
     if (a>300) or (len(hsl)==count):
      break
   except Exception as e:
    pass
 except Exception as e:
  pass
 """
 for i in range(1,7):
  if len(hsl)==count:
    break
  try:
   u='https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-'+str(i)
   if logs==True:
    print("[+]Checking: {}".format(u))
   c=requests.get(u,timeout=timeout).text
   soup = BeautifulSoup(c,"html.parser")
   y= soup.find_all("tr")
   for x in y:
    try:
     z = x.find_all('td')
     if "strong" in str(z):
       pass 
     else:
      ip= str(z[1]).split('<td>')[1].split('=')[0]
      ip=ip.split('</td>')[0].split('=')[0]
      p= str(z[2]).split('<td>')[1].split('=')[0]
      p=p.split('</td>')[0].split('=')[0]
      pr=ip+':'+p
      if pr not in hsl:
       hsl.append(pr)
       if len(hsl)==count:
         break
    except Exception as e:
     pass
  except Exception as e:
   pass
 """ 
 ur=["http://www.gatherproxy.com/proxylist/anonymity/?t=Elite","http://www.gatherproxy.com/proxylist/anonymity/?t=Anonymous"]
 for u in ur:
  if len(hsl)==count:
    break
  try:
   if logs==True:
    print("[+]Checking: {}".format(u))
   y=[]
   c=requests.get(u,timeout=timeout).text
   soup = BeautifulSoup(c,"html.parser")
   for r in soup.find_all("script"):
    h= ''.join(map(str, r.contents))
    if 'gp.insertPrx' in h:
     s = h.split(":")
     ip= s[3].split(",")[0].replace('\"','')
     p = str(int(s[5].split(",")[0].replace('\"',''), 16))
     pr=ip+':'+p
     if pr not in hsl:
       hsl.append(pr)
       if len(hsl)==count:
        break
  except Exception as e:
   pass
 return hsl
def https(logs=True,count=200,timeout=15):
 '''
   this function gather up hundreds of HTTPS proxies from many sources.
   those proxies are recommended to be used as reliable ones all the time.
   the function takes an argument which is the number of proxies to return (*args) , in case of no argument given it will
   return the whole list,and another argument (logs) which is set by default to True so you can see the process of the function, you can turn 
   off the the typing if you set it to: False.

   usage:

   >>>import bane
   >>>bane.https()

   >>>bane.https(200)   

'''
 dn='abcdefghijklmnopqrstuvwxyz'
 hl=[]
 u='https://www.sslproxies.org/'
 try:
  if logs==True:
   print("[+]Checking: {}".format(u))
  c=requests.get(u,timeout=timeout).text
  soup = BeautifulSoup(c,"html.parser")
  y= soup.find_all("tr")
  a=0
  for x in y:
   try:
    x=str(x)
    ip=x.split('</td>')[0].split('=')[0]
    ip=ip.split('<tr><td>')[1].split('=')[0]
    p=x.split('</td><td>')[1].split('=')[0]
    p=p.split('</td>')[0].split('=')[0]
    d=False
    for o in dn:
     if ((o in ip.lower()) or (o in p.lower())):
      d=True
    if d==False:
     pr=ip+':'+p
     a+=1
     if pr not in hl:
      hl.append(pr)
      if (a==200) or (len(hl)==count):
       a=0
       break
   except:
    pass
 except:
  pass
 hl=hl[0:len(hl)-4]
 """
 ur=['https://list.proxylistplus.com/ssl-List-1', 'https://list.proxylistplus.com/ssl-List-2']
 for u in ur:
  if len(hl)==count:
    break
  try:
   if logs==True:
    print("[+]Checking: {}".format(u))
   c=requests.get(u,timeout=timeout).text
   soup = BeautifulSoup(c,"html.parser")
   y= soup.find_all("tr")
   for x in y:
    try:
     z = x.find_all('td')
     if "strong" in str(z):
      pass 
     else:
       ip= str(z[1]).split('<td>')[1].split('=')[0]
       ip=ip.split('</td>')[0].split('=')[0]
       p= str(z[2]).split('<td>')[1].split('=')[0]
       p=p.split('</td>')[0].split('=')[0]
       d=False
       for o in dn:
        if ((o in ip.lower()) or (o in p.lower())):
         d=True
       if d==False:
        pr=ip+':'+p
        a+=1
        if pr not in hl:
         hl.append(pr)
         if (len(hl)==count):
          break
    except Exception as e:
     pass
  except Exception as e:
   pass
 """
 return hl
def socks5(logs=True,count=100,timeout=15):
 '''
   this function gather up hundreds of SOCKS5 proxies from many sources.
   those proxies are recommended to be used as reliable ones all the time.
   the function takes an argument which is the number of proxies to return (*args) , in case of no argument given it will
   return the whole list,and another argument (logs) which is set by default to True so you can see the process of the function, you can turn 
   off the the typing if you set it to: False.

   usage:

   >>>import bane
   >>>bane.socks5()

   >>>bane.socks5(50)
'''
 s5l=[]
 u='https://www.socks-proxy.net/'
 try:
  if logs==True:
   print("[+]Checking: {}".format(u))
  c=requests.get(u,timeout=timeout).text
  soup = BeautifulSoup(c,"html.parser")
  y= soup.find_all("tr")
  for x in y:
   if "Socks5" in str(x):
    try:
     x=str(x)
     ip=x.split('</td>')[0].split('=')[0]
     ip=ip.split('<tr><td>')[1].split('=')[0]
     p=x.split('</td><td>')[1].split('=')[0]
     p=p.split('</td>')[0].split('=')[0]
     pr=ip+':'+p
     if pr not in s5l:
      s5l.append(pr)
      if len(s5l)==count:
        break
    except:
     pass
 except:
   pass
 """
 ur=["https://list.proxylistplus.com/Socks-List-1"," https://list.proxylistplus.com/Socks-List-2"]
 for u in ur:
  if len(s5l)==count:
   break
  try:
   if logs==True:
    print("[+]Checking: {}".format(u))
   c=requests.get(u,timeout=timeout).text
   s = BeautifulSoup(c,"html.parser")
   y=s.find_all('tr')
   for x in y:
    try:
     z = x.find_all('td')
     if "strong" in str(z):
      pass 
     else:
      if "Socks5" in str(x):
       ip= str(z[1]).split('<td>')[1].split('=')[0]
       ip=ip.split('</td>')[0].split('=')[0]
       p= str(z[2]).split('<td>')[1].split('=')[0]
       p=p.split('</td>')[0].split('=')[0]
       pr=ip+':'+p
       if pr not in s5l:
        s5l.append(pr)
        if len(s5l)==count:
          break
    except Exception as e:
     pass
  except Exception as e:
   pass
 """
 return s5l
def socks4(logs=True,count=30,timeout=15):
 '''
   this function gather up hundreds of SOCKS4 proxies from many sources.
   those proxies are recommended to be used as reliable ones all the time.
   the function takes an argument which is the number of proxies to return (*args) , in case of no argument given it will
   return the whole list,and another argument (logs) which is set by default to True so you can see the process of the function, you can turn 
   off the the typing if you set it to: False.

   usage:

   >>>import bane
   >>>bane.socks4()

   >>>bane.socks4(50)
'''
 s4l=[]
 u='https://www.socks-proxy.net/'
 try:
  if logs==True:
   print("[+]Checking: {}".format(u))
  c=requests.get(u,timeout=timeout).text
  soup = BeautifulSoup(c,"html.parser")
  y= soup.find_all("tr")
  for x in y:
   if "Socks4" in str(x):
    try:
     x=str(x)
     ip=x.split('</td>')[0].split('=')[0]
     ip=ip.split('<tr><td>')[1].split('=')[0]
     p=x.split('</td><td>')[1].split('=')[0]
     p=p.split('</td>')[0].split('=')[0]
     pr=ip+':'+p
     if pr not in s4l:
      s4l.append(pr)
      if len(s4l)==count:
       break
    except Exception as e:
     pass
 except Exception as e:
  pass
 """ur=["https://list.proxylistplus.com/Socks-List-1"," https://list.proxylistplus.com/Socks-List-2"]
 for u in ur:
  if len(s4l)==count:
   break
  try:
   if logs==True:
    print("[+]Checking: {}".format(u))
   c=requests.get(u,timeout=timeout).text
   s = BeautifulSoup(c,"html.parser")
   y=s.find_all('tr')
   for x in y:
    try:
     z = x.find_all('td')
     if "strong" in str(z):
      pass 
     else:
      if "Socks4" in str(x):
       ip= str(z[1]).split('<td>')[1].split('=')[0]
       ip=ip.split('</td>')[0].split('=')[0]
       p= str(z[2]).split('<td>')[1].split('=')[0]
       p=p.split('</td>')[0].split('=')[0]
       pr=ip+':'+p
       if pr not in s4l:
        s4l.append(pr)
        if len(s4l)==count:
          break
    except Exception as e:
     pass
  except Exception as e:
   pass
 """
 return s4l
def proxy_check(ip,p,proto='http',timeout=5):
 '''
    this function is to check if the proxy is dead or not.

    it takes the following arguments:
    
    proto: (set by default to: http) the proxy type: http/https/socks4/socks5
    ip: proxy's ip
    p: proxy's port
    timeout: (set by default to: 5) the connection's timeout

'''
 i=False
 if (proto=='http') or (proto=='https'):
  try:
   requests.get("http://www.google.com",proxies={proto:'http://'+ip+':'+p},timeout=timeout)
   i=True
  except:
   pass
 elif proto=='socks4':
  try:
   s =socks.socksocket()
   s.setproxy(socks.PROXY_TYPE_SOCKS4, ip, p, True)
   s.settimeout(timeout)
   s.connect(('www.google.com',80))
   i=True
  except:
   pass
 elif proto=='socks5':
  try:
   s =socks.socksocket()
   s.setproxy(socks.PROXY_TYPE_SOCKS5, ip, p, True)
   s.settimeout(timeout)
   s.connect(('www.google.com',80))
   i=True
  except:
   pass
 return i
